# Step 1: Import and start Spark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("CollapseCasesTest") \
    .getOrCreate()

# Step 2: Load the CSV
input_df = spark.read.option("header", True).csv("sample_input.csv", inferSchema=True)
input_df.show()

# Step 3: Dummy helper functions
import pandas as pd
import time
from pyspark.sql.functions import lit

def get_prb_data(trans_df, real_day, cell_id, prb_column='PRB_UTILIZATION'):
    df = trans_df.toPandas()
    return {cell: list(df[df[cell_id] == cell][prb_column]) for cell in df[cell_id].unique()}

def run_link_threads(enb_subset, prb_dict, cell_id, real_day):
    return {enb: [(enb, prb_dict.get(enb, []))] for enb in enb_subset}

def collapse(filtered_data):
    collapsed_list = []
    for enb, records in filtered_data.items():
        for record in records:
            collapsed_list.append({
                "ENB_ID": record[0],
                "PRB_AVG": sum(record[1]) / len(record[1]) if record[1] else 0
            })
    return collapsed_list

optimised function 

def collapse_cases_with_correlation(
    trans_df,
    trans_dt,
    collapse_from_cache=False,
    mkt_filter=[],
    cell_id="SECTOR_ID",
    prb_column="PRB_UTILIZATION",
    enb_column="ENB_ID",
    day_column="DAY",
    collapse_output_loc=None,
    raw_output_loc=None,
    metrics_output_loc=None,
):
    start_time = time.time()
    
    trans_df = trans_df.withColumn(day_column, trans_df[day_column].cast("string"))
    trans_df = trans_df.withColumn(cell_id, trans_df[cell_id].cast("string"))

    trans_df = trans_df.filter(trans_df[day_column].isNotNull())
    real_day = str(trans_dt)

    if mkt_filter:
        trans_df = trans_df.filter(trans_df["MARKET_NAME"].isin(mkt_filter))

    trans_df = trans_df.withColumn(cell_id, trans_df[cell_id].cast("string"))

    prb_dict = get_prb_data(trans_df, real_day, cell_id, prb_column)

    enb_subset = list(set(trans_df.select(enb_column).rdd.flatMap(lambda x: x).collect()))

    filtered_data = run_link_threads(enb_subset, prb_dict, cell_id, real_day)

    result = collapse(filtered_data)

    for item in result:
        item[day_column] = real_day
        item["TOTAL"] = 1

    result_df = spark.createDataFrame(pd.DataFrame(result))

    if collapse_output_loc:
        result_df.write.mode("overwrite").parquet(collapse_output_loc)

    print(f"Collapse completed in {round(time.time() - start_time, 2)} seconds.")
    return result

run the function 

collapsed_result = collapse_cases_with_correlation(
    trans_df=input_df,
    trans_dt="2024-04-01",  # You can also try "2024-04-02"
    collapse_from_cache=False,
    mkt_filter=[],  # Add filter if needed
    cell_id="SECTOR_ID",
    prb_column="PRB_UTILIZATION",
    enb_column="ENB_ID",
    day_column="DAY"
)

print("Collapsed Results:")
for row in collapsed_result:
    print(row)




